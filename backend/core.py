import os
from typing import Any, Dict, List

from dotenv import load_dotenv
from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import ToolMessage
from langchain.tools import tool
from langchain_pinecone import PineconeVectorStore
from langchain_openai import OpenAIEmbeddings

load_dotenv(override=True)

embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))

vectorstore = PineconeVectorStore(index_name=os.getenv("INDEX_NAME"), embedding=embeddings)

model = init_chat_model("gpt-5.2", model_provider="openai")

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve relevant information to help answer a user's queries"""
    retrieved_data = vectorstore.as_retriever().invoke(query, k=100)

    serialized_data = "\n\n".join(
        f"Source: {doc.metadata.get('source', 'Unknown')}\n\nContent: {doc.page_content}" for doc in retrieved_data
    )

    return serialized_data, retrieved_data

def run_llm(query: str) -> Dict[str, Any]:
    """
    Run the RAG pipeline to answer a query using retrieved documentation

    Args:
        query (str): The user's question

    Returns:
        Dictionary containing:
            - answer: The generated answer
            - context: List of retrieved documents
    """

    system_prompt = (
        "You are a helpful AI assistant that answers user's questions. "
        "You have access to a tool that retrieves relevant data."
        "Use the tool to find the relevant information before answering questions."
        "Always cite the sources you use in the answers."
        "If you cannot find the answer in the retrieved data, say so"
    )

    agent = create_agent(model=model, system_prompt=system_prompt, tools=[retrieve_context])

    messages = [{"role": "user", "content": query}]

    response = agent.invoke({"messages": messages})

    answer = response["messages"][-1].content

    context_docs = []

    for message in response["messages"]:
        if isinstance(message, ToolMessage) and hasattr(message, "artifact"):
            if isinstance(message.artifact, list):
                context_docs.append(message.artifact)

    return {"answer": answer, "context": context_docs}

if __name__ == '__main__':
    result = run_llm("Do you have any information on Ved Muthal?")
    print(result)